# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kR2EVCLSxWPZO0drm7EXHGFeOruq9tAX
"""

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import cv2
import numpy as np
import logging
import os
import zipfile
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from IPython.display import Image, display

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Constants
FRAME_HEIGHT = 224
FRAME_WIDTH = 224
NUM_FRAMES = 40
BATCH_SIZE = 16
CLASS_LABELS = ['cover', 'defense', 'flick', 'hook', 'late_cut',
                'lofted', 'pull', 'square_cut', 'straight', 'sweep']

# Image transformation for frame extraction
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# TestVisualizer class
class TestVisualizer:
    def __init__(self, save_dir='/kaggle/working/test_plots'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        self.class_names = ['cover', 'defense', 'flick', 'hook', 'late_cut',
                           'lofted', 'pull', 'square_cut', 'straight', 'sweep']

    def plot_probabilities(self, video_name, probs):
        # Infer shot type from video name
        shot_type = None
        video_name_lower = video_name.lower()
        for label in self.class_names:
            if label in video_name_lower:
                shot_type = label
                break
        if shot_type is None:
            shot_type = "Unknown"

        plt.figure(figsize=(10, 6))
        plt.bar(self.class_names, probs, color='skyblue')
        plt.xlabel('Shot Type')
        plt.ylabel('Probability')
        plt.title(f'Class Probabilities for {shot_type}')
        plt.xticks(rotation=45)
        plt.tight_layout()
        safe_name = video_name.replace('/', '_').replace('.', '_')
        plot_path = os.path.join(self.save_dir, f'probs_{safe_name}.png')
        plt.savefig(plot_path)
        plt.close()
        return plot_path

    def plot_confusion_matrix(self, true_labels, pred_labels):
        cm = confusion_matrix(true_labels, pred_labels)
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names, yticklabels=self.class_names)
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.title('Test Set Confusion Matrix')
        plt.tight_layout()
        plot_path = os.path.join(self.save_dir, 'test_confusion_matrix.png')
        plt.savefig(plot_path)
        plt.close()
        return plot_path

# Define the CricketShotClassifier
class CricketShotClassifier(nn.Module):
    def __init__(self, input_size=2048, hidden_size=256, num_layers=2, num_classes=10, dropout=0.3):
        super(CricketShotClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Bidirectional LSTM
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True,
                           bidirectional=True,
                           dropout=dropout if num_layers > 1 else 0)

        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),  # *2 for bidirectional
            nn.Tanh(),
            nn.Linear(128, 1)
        )

        self.fc = nn.Sequential(
            nn.BatchNorm1d(hidden_size * 2),  # *2 for bidirectional
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )

    def forward(self, x, seq_lens):
        # Sort by sequence length for packed sequence
        sort_idx = torch.argsort(seq_lens, descending=True)
        x_sorted = x[sort_idx]
        seq_lens_sorted = seq_lens[sort_idx]

        # Pack, run LSTM, and unpack
        packed_x = pack_padded_sequence(x_sorted, seq_lens_sorted.cpu(), batch_first=True, enforce_sorted=True)
        packed_out, (hidden, _) = self.lstm(packed_x)
        output, _ = pad_packed_sequence(packed_out, batch_first=True)

        # Apply attention
        batch_size = output.size(0)
        weighted_outputs = []

        for i in range(batch_size):
            seq_len = seq_lens_sorted[i]
            attention_score = self.attention(output[i, :seq_len]).squeeze(-1)
            attention_score = torch.softmax(attention_score, dim=0)
            weighted_output = torch.matmul(attention_score.unsqueeze(0), output[i, :seq_len])
            weighted_outputs.append(weighted_output)

        # Concatenate outputs
        out = torch.cat(weighted_outputs, dim=0)

        # Unsort
        unsort_idx = torch.argsort(sort_idx)
        out = out[unsort_idx]

        # Final classification
        out = self.fc(out)
        return out

def extract_frames(video_path, num_frames=NUM_FRAMES):
    """
    Extract frames from a video.
    If the video has fewer frames than num_frames, duplicate the last frame.

    Args:
        video_path (str): Path to the video file.
        num_frames (int): Number of frames to extract.

    Returns:
        tuple: (frames: Tensor of shape (num_frames, 3, FRAME_HEIGHT, FRAME_WIDTH),
                sequence_length: Actual number of frames extracted before padding)
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        logging.error(f"Failed to open video: {video_path}")
        raise ValueError(f"Cannot open video: {video_path}")

    frames = []
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if frame_count <= num_frames:
        indices = list(range(frame_count))
    else:
        indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)

    for i in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = transform(frame)
            frames.append(frame)

    cap.release()
    sequence_length = len(frames)

    while len(frames) < num_frames:
        frames.append(frames[-1] if frames else torch.zeros(3, FRAME_HEIGHT, FRAME_WIDTH))

    frames = frames[:num_frames]
    return torch.stack(frames), sequence_length

def extract_features(frames, batch_size=BATCH_SIZE):
    """
    Extract features from frames using a pretrained ResNet50 (without its final layer).

    Args:
        frames (Tensor): Tensor of shape (num_frames, 3, FRAME_HEIGHT, FRAME_WIDTH).
        batch_size (int): Batch size for feature extraction.

    Returns:
        Tensor: Features of shape (num_frames, 2048).
    """
    feature_extractor = models.resnet50(pretrained=True)
    feature_extractor = torch.nn.Sequential(*list(feature_extractor.children())[:-1])
    feature_extractor.eval().to(device)

    features = []
    with torch.no_grad():
        for i in range(0, len(frames), batch_size):
            batch = frames[i:i+batch_size].to(device)
            batch_features = feature_extractor(batch)
            batch_features = batch_features.squeeze(-1).squeeze(-1)
            features.append(batch_features.cpu())

    return torch.cat(features, dim=0)

def infer_true_label(video_name):
    """
    Infer the true label from the video file name (e.g., 'cover_087.avi' -> 'cover').

    Args:
        video_name (str): Name of the video file.

    Returns:
        int: Index of the true label in CLASS_LABELS, or None if not found.
    """
    video_name = video_name.lower()
    for idx, label in enumerate(CLASS_LABELS):
        if label in video_name:
            return idx
    logging.warning(f"Could not infer true label for {video_name}")
    return None

def predict_shot(zip_path, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    Predict the cricket shot type for all .avi videos in a zip file and visualize results.

    Args:
        zip_path (str): Path to the zip file containing a videos folder.
        model_path (str): Path to the trained model checkpoint.
        device (str): Device to run the model on ('cuda' or 'cpu').

    Returns:
        tuple: (predictions: dict mapping video names to (predicted_class_name, confidence_score),
                plot_paths: dict mapping video names to probability plot paths,
                cm_plot_path: path to confusion matrix plot or None)
    """
    device = torch.device(device)

    # Initialize visualizer
    visualizer = TestVisualizer()

    # Extract zip file
    extract_dir = '/kaggle/working/videos_extracted'
    os.makedirs(extract_dir, exist_ok=True)

    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    # Find .avi files
    video_files = []
    for root, _, files in os.walk(extract_dir):
        for file in files:
            if file.endswith('.avi'):
                video_files.append(os.path.join(root, file))

    if not video_files:
        logging.error("No .avi files found in the zip.")
        raise ValueError("No .avi files found in the zip.")

    logging.info(f"Found {len(video_files)} .avi videos for prediction.")

    # Load the model
    model = CricketShotClassifier(input_size=2048, hidden_size=256, num_layers=2,
                                 num_classes=len(CLASS_LABELS), dropout=0.3)
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    # Predict shot types and collect labels for confusion matrix
    predictions = {}
    plot_paths = {}
    true_labels = []
    pred_labels = []

    for video_path in video_files:
        video_name = os.path.basename(video_path)
        try:
            # Extract frames
            frames, seq_len = extract_frames(video_path)

            # Extract features
            features = extract_features(frames)

            # Prepare input for model
            features = features.unsqueeze(0)  # Add batch dimension: (1, NUM_FRAMES, 2048)
            seq_len = torch.tensor([seq_len], dtype=torch.long).to(device)  # Sequence length as tensor

            # Make prediction
            with torch.no_grad():
                features = features.to(device)
                output = model(features, seq_len)
                probabilities = torch.softmax(output, dim=1)
                confidence, predicted_idx = torch.max(probabilities, dim=1)

                predicted_class = CLASS_LABELS[predicted_idx.item()]
                confidence_score = confidence.item()
                probs = probabilities.squeeze().cpu().numpy()

            # Store prediction
            predictions[video_name] = (predicted_class, confidence_score)

            # Plot probabilities and display
            plot_path = visualizer.plot_probabilities(video_name, probs)
            plot_paths[video_name] = plot_path
            display(Image(filename=plot_path))

            # Infer true label for confusion matrix
            true_label_idx = infer_true_label(video_name)
            if true_label_idx is not None:
                true_labels.append(true_label_idx)
                pred_labels.append(predicted_idx.item())

            logging.info(f"Video: {video_name}, Predicted shot: {predicted_class}, Confidence: {confidence_score:.4f}, Plot: {plot_path}")

        except Exception as e:
            predictions[video_name] = (f"Error: {str(e)}", 0.0)
            plot_paths[video_name] = None
            logging.error(f"Failed to process {video_path}: {str(e)}")

    # Plot confusion matrix if true labels are available
    cm_plot_path = None
    if true_labels:
        cm_plot_path = visualizer.plot_confusion_matrix(true_labels, pred_labels)
        logging.info(f"Confusion matrix saved at: {cm_plot_path}")
        display(Image(filename=cm_plot_path))

    return predictions, plot_paths, cm_plot_path

# Example usage
if __name__ == "__main__":
    # Zip file path (downloaded from Google Drive)
    #You can pass here directly 
    # zip_path = "/kaggle/working/videos.zip"
    # model_path = "final_model.pth"

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_zip', type=str, required=True, 
                       help='Path to input videos zip file')
    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model weights')
    args = parser.parse_args()

    try:
        # Download zip file from Google Drive
        import gdown
        # gdown.download(id="1dQtjVf_T0gH8SXjXGVItyRTYGgXtAU-t", output=zip_path, quiet=False)

        # # Predict shot types and visualize
        # predictions, plot_paths, cm_plot_path = predict_shot(zip_path, model_path)
        gdown.download(id="1dQtjVf_T0gH8SXjXGVItyRTYGgXtAU-t", 
                      output=args.input_zip, quiet=False)
        
        # Call predict_shot with arguments
        predictions, plot_paths, cm_plot_path = predict_shot(args.input_zip, args.model_path)

        # Print predictions
        print("\nPredictions:")
        for video_name, (shot_type, confidence) in predictions.items():
            plot_path = plot_paths.get(video_name, "No plot")
            if shot_type.startswith("Error"):
                print(f"Video: {video_name}, {shot_type}, Plot: {plot_path}")
            else:
                print(f"Video: {video_name}, Predicted shot: {shot_type}, Confidence: {confidence:.4f}, Plot: {plot_path}")

        if cm_plot_path:
            print(f"Confusion matrix: {cm_plot_path}")

    except Exception as e:
        print(f"Error during prediction: {str(e)}")